# -*- coding: utf-8 -*-
"""project_sales.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18FeHn6FPwEgZGtgTzSYFJvBPdCWrCx95
"""

!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget https://archive.apache.org/dist/spark/spark-3.2.2/spark-3.2.2-bin-hadoop3.2.tgz
!tar xf spark-3.2.2-bin-hadoop3.2.tgz
!pip install -q findspark pandas numpy

# configurar as variáveis de ambiente
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.2-bin-hadoop3.2"

# tornar o pyspark "importável"
import findspark
findspark.init('spark-3.2.2-bin-hadoop3.2')


from pyspark.sql import functions as func
from pyspark.sql import SparkSession
from pyspark.sql import types as T
from pyspark.sql import functions as F
import pyspark.pandas as ps
import pandas as pd
import numpy as np
import warnings, re


warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)


# Instanciando Spark
spark = SparkSession.builder.master('local[*]').getOrCreate()

import datetime

import seaborn as sns

from pyspark.sql.functions import monotonically_increasing_id

path = "/content/Base de Dados - Questionário.xlsx - Sheet1.csv"

df = spark.read.format("csv").option("header", True).option("ignoreLeadingWhiteSpace", True).option("inferSchema", True).options(delimiter=',').load(path)

df.printSchema()

df_index = df.select("*").withColumn("id", monotonically_increasing_id())

filtro = df_index['id'] > 2

df_novo = df_index.filter(filtro)

df_novo.select('_c0','_c1').distinct().show()

df_novo = df_novo.drop('_c0','_c1')

df_novo1 = df_novo.withColumnRenamed("_c2","idem")\
        .withColumnRenamed("_c3","product_id")\
        .withColumnRenamed("_c4","client_id")\
        .withColumnRenamed("_c5","discount")\
        .withColumnRenamed("_c6","unit_price")\
        .withColumnRenamed("_c7","quantity")\
        .withColumnRenamed("_c8","store_id")\
        .withColumnRenamed("_c9","date")\
        .withColumnRenamed("id","id_temp")

df_novo1 = df_novo1.filter(df_novo1.id_temp != 3)

df_novo1.groupBy("product_id").count().orderBy(F.col('count').desc()).show(truncate=False)

df_novo = df_novo.filter(filtro)

df_contagem = df_novo1.select("product_id").distinct().count()

df_contagem

df_contagem2 = df_novo1.groupBy("product_id")

df_5 = (
    df_novo1.withColumn('ano', F.substring('date', 1, 4).cast('integer'))
    )

df_ir= df_novo1.filter(df_novo1.date.contains('2019'))

pa_pf = df_ir.toPandas()

pa_pf["quantity"] = pd.to_numeric(pa_pf["quantity"])

pa_pf["date"] = pd.to_datetime(pa_pf["date"])

pa_pf["unit_price"] = pa_pf["unit_price"].str.replace(',','.').astype(float)

pa_pf["discount"] = pa_pf["discount"].str.replace(',','.').astype(float)

pa_pf['mes'] = pa_pf['date'].dt.month

pa_pf.head()

pa_pf.info()

pa_pf['liquida'] = (pa_pf['quantity'] * pa_pf['unit_price']) - pa_pf['discount'] * (pa_pf['quantity'] * pa_pf['unit_price'])

pa_pf.head()

df_final = pa_pf[['mes','unit_price', 'liquida', 'quantity']].groupby('mes').sum().reset_index()

df_final.head()

df_final.describe()

df_final_09 = pa_pf[['store_id', 'unit_price', 'liquida', 'quantity']].groupby('store_id').sum().reset_index()

df_final_09.describe()

display(df_final_09)

type(pa_pf['discount'])

df_final['liquida'] = (df_final['quantity'] * df_final['unit_price']) - df_final['discount'] * (df_final['quantity'] * df_final['discount'])

df_final.head()

df.mean(pa_pf) #Por padrão o mean no pandas faz a média das linhas

pa_pf.sort_values(by = 'date', inplace = True)

df_final.describe()

df_ir.show()

df_5.show()

df_novo1.show(20)

path2 = "/content/sheet_2.xlsx - Sheet2.csv"

df2 = spark.read.format("csv").option("header", True).option("ignoreLeadingWhiteSpace", True).option("inferSchema", True).options(delimiter=',').load(path2)

df2.printSchema()

id_marca = '002ec297b1b00fb9dde7ee6ac24b67713'

df2.where(df2.Name == 'Iron Man').show(truncate=False)

df2.show(20, truncate=False)

path3 = "/content/sheet_3.xlsx - Sheet3.csv"

df3 = spark.read.format("csv").option("header", True).option("ignoreLeadingWhiteSpace", True).option("inferSchema", True).options(delimiter=',').load(path3)

df3.printSchema()

df3.show(20)

path4 = "/content/sheet_4.xlsx - Sheet4.csv"

df4 = spark.read.format("csv").option("header", True).option("ignoreLeadingWhiteSpace", True).option("inferSchema", True).options(delimiter=',').load(path4)

df4.printSchema()

df4.show(20)